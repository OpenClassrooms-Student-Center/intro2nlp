{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "[![Open In Colab](colab-badge.svg)](https://colab.research.google.com/github/alexisperrier/intro2nlp/blob/master/notebooks/intro2nlp_03_tokenization.ipynb)\n",
    "\n",
    "\n",
    "Tokenization or [Text segmentation](https://en.wikipedia.org/wiki/Text_segmentation) is the problem of dividing a string of written language into its component words. \n",
    "\n",
    "The most simple way to divide a text into a list of its words is to split over the whitespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'eat,', 'grandpa']\n"
     ]
    }
   ],
   "source": [
    "text = \"Let's eat, grandpa\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with that approach is that contractions (Let's -> Let + s) are not handled and punctuations signs stay attached to the nearest word (eat, -> eat + ,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right way to tokenize is to use a tokenizer. Most NLP libraries offer their own tokenizers. Here we will use tokenizers from the [NLTK](https://www.nltk.org/) library.\n",
    "\n",
    "The NLTK library offers many [tokenizers](https://www.nltk.org/api/nltk.tokenize.html). We'll work with the WordPunctTokenizer.\n",
    "\n",
    "But first let's install NLTK and download the necessary resources.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/alexis/anaconda3/envs/amcp/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: click in /Users/alexis/anaconda3/envs/amcp/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /Users/alexis/anaconda3/envs/amcp/lib/python3.8/site-packages (from nltk) (4.49.0)\n",
      "Requirement already satisfied: joblib in /Users/alexis/anaconda3/envs/amcp/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in /Users/alexis/anaconda3/envs/amcp/lib/python3.8/site-packages (from nltk) (2020.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/alexis/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk \n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the WordPunctTokenizer\n",
    "\n",
    "We get a different results. The punctuations are now handled as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'\", 's', 'eat', 'your', 'soup', ',', 'Grandpa', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokens = WordPunctTokenizer().tokenize(\"Let's eat your soup, Grandpa.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the text from the Wikipedia Earth page and look at the frequency of the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "def wikipedia_page(title):\n",
    "    '''\n",
    "    This function returns the raw text of a wikipedia page \n",
    "    given a wikipedia page title\n",
    "    '''\n",
    "    params = { \n",
    "        'action': 'query', \n",
    "        'format': 'json', # request json formatted content\n",
    "        'titles': title, # title of the wikipedia page\n",
    "        'prop': 'extracts', \n",
    "        'explaintext': True\n",
    "    }\n",
    "    # send a request to the wikipedia api \n",
    "    response = requests.get(\n",
    "         'https://en.wikipedia.org/w/api.php',\n",
    "         params= params\n",
    "     ).json()\n",
    "\n",
    "    # Parse the result\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    # return the page content \n",
    "    if 'extract' in page.keys():\n",
    "        return page['extract']\n",
    "    else:\n",
    "        return \"Page not found\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 672), (',', 525), ('.', 455), ('of', 318), ('and', 239), ('earth', 206), ('is', 169), ('to', 154), ('in', 147), ('a', 126), ('s', 120), (\"'\", 119), ('(', 99), ('-', 76), ('by', 72), ('from', 69), ('that', 68), ('as', 67), ('at', 57), ('with', 52)]\n"
     ]
    }
   ],
   "source": [
    "text = wikipedia_page('Earth').lower()\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "print(Counter(tokens).most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that earth and earth's for instance are no longer separate tokens and that the punctuation signs are stand alone tokens. This will come in handy if we want to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization on characters\n",
    "\n",
    "We can also tokenize on characters instead of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common characters in the text\n",
      "[(' ', 8005), ('e', 4931), ('t', 3855), ('a', 3640), ('i', 2954), ('o', 2921), ('s', 2715), ('r', 2661), ('n', 2645), ('h', 1944), ('l', 1826), ('c', 1414), ('d', 1270), ('m', 1169), ('u', 997), ('p', 841), ('f', 837), ('g', 719), ('y', 592), (',', 568)]\n",
      "\n",
      "All characters in the text: \n",
      "{' ', 'f', 'm', '9', '5', 'b', 'k', 'r', '4', '−', ':', 'ñ', '1', 'w', 'ō', '.', '+', 'c', ',', '0', '–', 'h', 'č', '°', 'g', 'þ', '3', ';', 'v', 'z', 'æ', '(', '/', '%', '*', 'i', '-', 'q', 'p', 't', 's', 'u', 'ῆ', '8', '=', 'ð', 'l', 'ū', 'ē', 'ć', '?', 'ʻ', 'n', 'o', \"'\", 'a', '\\n', '—', 'á', '7', 'j', 'e', '6', ')', 'ö', '\"', '×', 'x', '±', 'γ', 'd', 'µ', 'y', '2'}\n"
     ]
    }
   ],
   "source": [
    "# example of character tokenization\n",
    "char_tokens = [ c for c in text ]\n",
    "print(\"Most common characters in the text\")\n",
    "print(Counter(char_tokens).most_common(20))\n",
    "print()\n",
    "print(f\"All characters in the text: \\n{set(char_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "\n",
    "Some words are better taken together: New York, Happy end, Wall street, Linear regression etc ... . When tokenizing we want to consider all possible adjacent pairs of words in the text. We can do this with the NLTK ngrams function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'much'), ('much', 'wood'), ('wood', 'would'), ('would', 'a'), ('a', 'woodchuck'), ('woodchuck', 'chuck'), ('chuck', 'if'), ('if', 'a'), ('a', 'woodchuck'), ('woodchuck', 'could'), ('could', 'chuck'), ('chuck', 'wood'), ('wood', '?')]\n",
      "\n",
      "['how_much', 'much_wood', 'wood_would', 'would_a', 'a_woodchuck', 'woodchuck_chuck', 'chuck_if', 'if_a', 'a_woodchuck', 'woodchuck_could', 'could_chuck', 'chuck_wood', 'wood_?']\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "text = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\".lower()\n",
    "\n",
    "# Tokenize\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "# bigrams \n",
    "bigrams = [w for w in  ngrams(tokens,n=2)]\n",
    "print(bigrams)\n",
    "\n",
    "print()\n",
    "bigrams = ['_'.join(bg) for bg in bigrams]\n",
    "print(bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how_much_wood', 'much_wood_would', 'wood_would_a', 'would_a_woodchuck', 'a_woodchuck_chuck', 'woodchuck_chuck_if', 'chuck_if_a', 'if_a_woodchuck', 'a_woodchuck_could', 'woodchuck_could_chuck', 'could_chuck_wood', 'chuck_wood_?']\n"
     ]
    }
   ],
   "source": [
    "# and for trigrams\n",
    "\n",
    "trigrams = ['_'.join(w) for w in  ngrams(tokens,n=3)]\n",
    "\n",
    "print(trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add ngrams to list of tokens\n",
    "Let's add the bigrams and trigrams to the list of tokens on the wikipedia Earth page and look at the frequency of ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikipedia_page('Earth').lower()\n",
    "unigrams = WordPunctTokenizer().tokenize(text)\n",
    "bigrams = ['_'.join(w) for w in  ngrams(unigrams,n=2)]\n",
    "trigrams = ['_'.join(w) for w in  ngrams(unigrams,n=3)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = unigrams + bigrams + trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have a total of 30201 tokens, including: \n",
      "- 10068 unigrams \n",
      "- 10067 bigrams \n",
      "- 10066 trigrams. \n"
     ]
    }
   ],
   "source": [
    "print(f\"we have a total of {len(tokens)} tokens, including: \\n- {len(unigrams)} unigrams \\n- {len(bigrams)} bigrams \\n- {len(trigrams)} trigrams. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 672),\n",
       " (',', 525),\n",
       " ('.', 455),\n",
       " ('of', 318),\n",
       " ('and', 239),\n",
       " ('earth', 206),\n",
       " ('is', 169),\n",
       " ('to', 154),\n",
       " ('in', 147),\n",
       " ('a', 126),\n",
       " ('s', 120),\n",
       " (\"'\", 119),\n",
       " (\"'_s\", 119),\n",
       " ('(', 99),\n",
       " (\"earth_'\", 98),\n",
       " (\"earth_'_s\", 98),\n",
       " ('of_the', 87),\n",
       " ('._the', 80),\n",
       " ('-', 76),\n",
       " ('by', 72),\n",
       " ('from', 69),\n",
       " ('that', 68),\n",
       " ('as', 67),\n",
       " ('at', 57),\n",
       " (',_and', 56),\n",
       " ('with', 52),\n",
       " ('are', 52),\n",
       " ('sun', 49),\n",
       " ('surface', 49),\n",
       " (',_the', 49),\n",
       " ('in_the', 48),\n",
       " (')', 46),\n",
       " ('on', 45),\n",
       " ('about', 43),\n",
       " ('the_sun', 42),\n",
       " ('===', 40),\n",
       " ('of_earth', 40),\n",
       " ('to_the', 40),\n",
       " ('this', 39),\n",
       " ('million', 35),\n",
       " ('atmosphere', 34),\n",
       " ('1', 34),\n",
       " ('its', 34),\n",
       " ('life', 33),\n",
       " ('for', 32),\n",
       " ('moon', 32),\n",
       " ('solar', 31),\n",
       " ('%', 30),\n",
       " ('water', 30),\n",
       " ('years', 30)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have multiple bigrams in the top 50 tokens:\n",
    "\n",
    "- of_the\n",
    "- of_earth\n",
    "- in_the\n",
    "\n",
    "Adding ngrams to a list of tokens may help down the line when classifying text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
