{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply a Simple Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    '2 cups of flour',\n",
    "    'replace the flour',\n",
    "    'replace the keyboard in 2 minutes',\n",
    "    'do you prefer Windows or Mac',\n",
    "    'the Mac has the most noisy keyboard',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification\n",
    "\n",
    "The simplified brown corpus is available in the data folder of this github repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/brown_corpus_extract_humor_science_fiction.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>humor</td>\n",
       "      <td>They were always leaping to light cigarettes ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>science_fiction</td>\n",
       "      <td>No , the fish could eat their bodies for all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science_fiction</td>\n",
       "      <td>Fighting pleasure itself , he begged B'dikkat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science_fiction</td>\n",
       "      <td>that is all I can tell you about them .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>humor</td>\n",
       "      <td>`` That's one thing I've never done '' , she s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic                                               text\n",
       "0            humor  They were always leaping to light cigarettes ,...\n",
       "1  science_fiction  No , the fish could eat their bodies for all t...\n",
       "2  science_fiction  Fighting pleasure itself , he begged B'dikkat ...\n",
       "3  science_fiction            that is all I can tell you about them .\n",
       "4            humor  `` That's one thing I've never done '' , she s..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that \n",
    "\n",
    "- removes stopwords\n",
    "- removes punctuation signs\n",
    "- lemmatizes the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not (token.is_stop or token.is_punct)]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good time leave care\n"
     ]
    }
   ],
   "source": [
    "text = '''These are the good times; Leave your cares behind'''\n",
    "print(preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a couple of punctuation signs with multi charaters as stopwords to the ```nlp``` spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add(\"`,\")\n",
    "nlp.Defaults.stop_words.add(\"``\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our preprocess function to the simplified Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df.text.apply(lambda txt : preprocess(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short text with just a few tokens won't have enough information for the classification model that we want to train. Let's add a count of the number of tokens for each text and filter out the text with less than N tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_count'] = df.processed_text.apply(lambda txt : len(txt.split())  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2001.000000\n",
       "mean        7.324338\n",
       "std         5.760664\n",
       "min         0.000000\n",
       "25%         3.000000\n",
       "50%         6.000000\n",
       "75%        10.000000\n",
       "max        52.000000\n",
       "Name: token_count, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.token_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.token_count.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove texts that have less than 4 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.token_count > 4]\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humor              738\n",
       "science_fiction    520\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.token_count > 4].topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df.processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now a sparse matrix of 1258 rows by 4749 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 4768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the topic from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[df.topic == 'humor', 'topic' ] = 0\n",
    "df.loc[df.topic == 'science_fiction', 'topic' ] = 1\n",
    "df.topic = df.topic.astype(int)\n",
    "# define the target variable\n",
    "y = df.topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1253    0\n",
       "1254    0\n",
       "1255    1\n",
       "1256    1\n",
       "1257    1\n",
       "Name: topic, Length: 1258, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1258x4768 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11620 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9888712241653418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Declare the model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 3. Make predictions \n",
    "yhat = clf.predict(X)\n",
    "\n",
    "# 4. score\n",
    "print(\"Accuracy: \",accuracy_score(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really train a classifier, we need to split the data in two parts: a training subset on which we train the model and a test set on which we evaluate the model. \n",
    "\n",
    "The test set simulates data that the model has not seen during its training and gives us a way to measure how the model extrapolates on unseen data.\n",
    "\n",
    "We need to split the data after we've vectorized the text, otherwise sont tokens may be present in the test set but not in the training set. \n",
    "\n",
    "To split the data into a train and a test set we use scikit's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(842, 4768)\n",
      "(416, 4768)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the same model, this time on the train set and evaluate it on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7740384615384616\n"
     ]
    }
   ],
   "source": [
    "# 1. Declare the model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions  on test set\n",
    "yhat = clf.predict(X_test)\n",
    "\n",
    "# 4. score\n",
    "print(\"Accuracy: \",accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is down to 77.4% which makes more sense than the outstanding 98% we previously obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "Let's now compare the model performance when we use a tf-idf vectorization approach instead of a simple tf / count vectorizer.\n",
    "\n",
    "The code is similar and we use scikit's default parameter for [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7668269230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(df.processed_text)\n",
    "y = df.topic\n",
    "\n",
    "# split test train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train and evaluate the model\n",
    "# 1. Declare the model\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions  on test set\n",
    "yhat = clf.predict(X_test)\n",
    "\n",
    "# 4. score\n",
    "print(\"Accuracy: \",accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, not much difference between the 2 vectorizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# term to term matrix\n",
    "\n",
    "Here is the code to generate the term to term matrix in chapter 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['install', 'keyboard', 'license', 'mac', 'need', 'noisy', 'prefer', 'replace', 'ways', 'windows'] \n",
      "\n",
      "Each sentence in token form:\n",
      " [['ways', 'to', 'replace', 'the', 'noisy', 'Mac', 'keyboard'], ['do', 'you', 'prefer', 'Windows', 'or', 'Mac'], ['the', 'Mac', 'has', 'a', 'noisy', 'keyboard'], ['ways', 'to', 'install', 'Windows', 'on', 'a', 'Mac'], ['you', 'need', 'a', 'Windows', 'license', 'to', 'install', 'Windows', 'on', 'a', 'Mac']] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>install</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>license</th>\n",
       "      <th>mac</th>\n",
       "      <th>need</th>\n",
       "      <th>noisy</th>\n",
       "      <th>prefer</th>\n",
       "      <th>replace</th>\n",
       "      <th>ways</th>\n",
       "      <th>windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mac</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ways</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windows</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>license</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noisy</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>replace</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>install</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          install  keyboard  license  mac  need  noisy  prefer  replace  ways  \\\n",
       "mac             2         2        1    0     0      2       1        1     2   \n",
       "ways            1         0        0    2     0      1       0        1     0   \n",
       "windows         3         0        2    3     1      0       1        0     1   \n",
       "need            1         0        1    0     0      0       0        0     0   \n",
       "license         1         0        0    1     1      0       0        0     0   \n",
       "noisy           0         2        0    2     0      0       0        1     1   \n",
       "replace         0         1        0    1     0      1       0        0     1   \n",
       "install         0         0        1    2     1      0       0        0     1   \n",
       "keyboard        0         0        0    2     0      2       0        1     0   \n",
       "prefer          0         0        0    1     0      0       0        0     0   \n",
       "\n",
       "          windows  \n",
       "mac             3  \n",
       "ways            1  \n",
       "windows         0  \n",
       "need            1  \n",
       "license         2  \n",
       "noisy           0  \n",
       "replace         0  \n",
       "install         3  \n",
       "keyboard        0  \n",
       "prefer          1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "# The corpus\n",
    "sentences = ['ways to replace the noisy Mac keyboard',\n",
    "'do you prefer Windows or Mac',\n",
    "'the Mac has a noisy keyboard',\n",
    "'ways to install Windows on a Mac',\n",
    "'you need a Windows license to install Windows on a Mac'\n",
    "]\n",
    "\n",
    "# vocab without stopwords\n",
    "vocab = sorted(list(set(word_tokenize(' '.join(sentences)))))\n",
    "stopwords = ['a', 'do', 'has', 'in', 'on', 'or', 'the', 'to', 'you', 'your']\n",
    "vocab = sorted([tk.lower() for tk in vocab if tk not in stopwords])\n",
    "\n",
    "print('Vocabulary:\\n',vocab,'\\n')\n",
    "\n",
    "# tokenize\n",
    "token_sent_list = [word_tokenize(sen) for sen in sentences]\n",
    "print('Each sentence in token form:\\n',token_sent_list,'\\n')\n",
    "\n",
    "# co occurrence window\n",
    "k=3\n",
    "\n",
    "# Definitely not an elegant way to create the term to term matrix\n",
    "co_occ = {ii:Counter({jj:0 for jj in vocab if jj!=ii}) for ii in vocab}\n",
    "\n",
    "\n",
    "for sen in token_sent_list:\n",
    "    sen = [tk.lower() for tk in sen if tk not in stopwords]\n",
    "    for ii in range(len(sen)):\n",
    "        if ii < k:\n",
    "            c = Counter(sen[0:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        elif ii > len(sen)-(k+1):\n",
    "            c = Counter(sen[ii-k::])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        else:\n",
    "            c = Counter(sen[ii-k:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "\n",
    "# Having final matrix in dict form lets you convert it to different python data structures\n",
    "co_occ = {ii:dict(co_occ[ii]) for ii in vocab}\n",
    "\n",
    "# convert to DataFrame\n",
    "\n",
    "df = pd.DataFrame(co_occ)\n",
    "df.fillna(0, inplace = True)\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "# here goes\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
